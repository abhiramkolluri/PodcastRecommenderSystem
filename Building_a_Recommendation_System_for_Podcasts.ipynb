{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6wvcIiGiRIO"
   },
   "source": [
    "# Building a Recommendation System for Podcasts\n",
    "\n",
    "Final Project - APAM Senior Seminar - Fall 2022 - October 10\n",
    "\n",
    "Yamini Ananth, Jafar Vohra, Kathy Wang, Abhiram Kolluri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qAueAHoBAf_",
    "outputId": "9507c2e5-66eb-41a1-ce6c-1f15fdb76c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stop-words\n",
      "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
      "Building wheels for collected packages: stop-words\n",
      "  Building wheel for stop-words (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32916 sha256=c2c8a20da4dd9a7b1c021b21274091be7f408bb6853e70297b3e3345e93af2df\n",
      "  Stored in directory: /Users/abhiramkolluri/Library/Caches/pip/wheels/eb/03/0d/3bd31c983789aeb0b4d5e2ca48590288d9db1586cf5f225062\n",
      "Successfully built stop-words\n",
      "Installing collected packages: stop-words\n",
      "Successfully installed stop-words-2018.7.23\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 132 kB/s eta 0:00:013███████              | 158.4 MB 6.5 MB/s eta 0:00:19     |█████████████████████████       | 220.0 MB 454 kB/s eta 0:02:16\n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 25.2 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845515 sha256=d026a3c910cf6aa96254594d2375371637869e68b27df3acb9e20f7fb28f33df\n",
      "  Stored in directory: /Users/abhiramkolluri/Library/Caches/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
     ]
    }
   ],
   "source": [
    "#installing modules\n",
    "!pip install stop-words\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6GlFcQu-ZXT",
    "outputId": "db39ef03-4407-4b09-e8bf-97252dd8d7e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abhiramkolluri/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abhiramkolluri/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/abhiramkolluri/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports for text pre-processing & data manipulation\n",
    "\n",
    "import itertools\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "#imports for collaborative filtering model\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "#utilities for text processing\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_rzY1P_-ZXW"
   },
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZg48Z_Rin_t"
   },
   "source": [
    "Data scraped from Apple Podcasts using BeautifulSoup\n",
    "\n",
    "Scripts for web scraping attributed to [Siddharth Kumaran](https://github.com/siddgood/podcast-recommendation-engine/blob/master/scripts/get_podcast_info.py)\n",
    "\n",
    "Scraped data includes: \n",
    "* Title (text)\n",
    "* Producer (text)\n",
    "* Description (text)\n",
    "* 6 Recent Episode Titles (text)\n",
    "* 6 Recent Episode Descriptions (text)\n",
    "\n",
    "Pre-processing included:\n",
    "* Filtered out URLs and special characters\n",
    "* Tokenized (separated each word into its own string)\n",
    "* Removed stop-words (common words like articles, pronouns etc)\n",
    "* Lemmatized (removed endings from words, so ‘like’ and ‘likes’ and ‘likely’ would all be converted to ‘lik’)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bjIzSS-3-ZXX"
   },
   "outputs": [],
   "source": [
    "podcasts_df_orig = pd.read_pickle('https://github.com/yaminivibha/podcast-recs/blob/main/data/data/pickle_files/english_podcasts_detailed_cleaned.pkl?raw=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "toiWYTPd-ZXY"
   },
   "outputs": [],
   "source": [
    "# Combining all text data into one column for downstream analysis\n",
    "\n",
    "podcasts_df = podcasts_df_orig\n",
    "podcasts_df['text'] = podcasts_df[['title', 'producer', 'genre', 'description', 'episode_titles', 'episode_descriptions']].apply(lambda x: ' '.join(x), axis=1)\n",
    "podcasts_df = podcasts_df.drop(columns=['genre', 'description', 'num_episodes', 'rating', 'num_reviews', 'link', 'episode_titles', 'episode_descriptions'])\n",
    "podcasts_df['idx'] = list(range(podcasts_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zq5qlO4--ZXY"
   },
   "outputs": [],
   "source": [
    "# Creating stopwords list & tokenizer\n",
    "\n",
    "stop = get_stop_words('en')\n",
    "stop = [re.sub(r'([^\\s\\w]|_)+', '', x) for x in stop]\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "M5ZyzMby-ZXZ"
   },
   "outputs": [],
   "source": [
    "# Creating helper functions to remove stop words \n",
    "# and lemmatize tokenized sentences\n",
    "\n",
    "def remove_stop(text, stop):\n",
    "    return [word for word in text if word not in stop ]\n",
    "\n",
    "def lemmatize(text, l_stemmer):\n",
    "    return [l_stemmer.lemmatize(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mFXFWqCG-ZXZ"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # remove mixed alphanumeric, URLS, stop words\n",
    "    text = re.sub(r\"\"\"(?x) \\b(?=\\w*\\d)\\w+\\s*\"\"\",\"\", text)\n",
    "    re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r'([^\\s\\w]|_)+', '', text)\n",
    "    text = tokenizer.tokenize(text.lower())\n",
    "    text = remove_stop(text, stop)\n",
    "    text = lemmatize(text, WordNetLemmatizer())\n",
    "    \n",
    "    new_text = ' '.join(text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wmihuN4m-ZXa"
   },
   "outputs": [],
   "source": [
    "podcasts_df['text'] = podcasts_df['text'].map(preprocess_text)\n",
    "podcasts_df = podcasts_df.query('text !=\"\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zvnwusX-ZXa"
   },
   "source": [
    "# Preparing Utilities for Recommendation\n",
    "\n",
    "Collection of helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7A3fQVdT-ZXa"
   },
   "outputs": [],
   "source": [
    "def get_title_from_index(index):\n",
    "    \"\"\"get title of podcast from index of podcast\n",
    "        parameters:\n",
    "            index: (int)\n",
    "        returns:\n",
    "            title (string)\n",
    "        raises:\n",
    "            ValueError: index not in podcasts_df['idx']\n",
    "    \"\"\"\n",
    "    return podcasts_df[podcasts_df.idx == index][\"title\"].values[0]\n",
    "\n",
    "def get_index_from_title(title):\n",
    "    \"\"\"get index of podcast from title of podcast\n",
    "        parameters:\n",
    "            title: (string)\n",
    "        returns:\n",
    "            index (int)\n",
    "        raises:\n",
    "            ValueError: string not in podcasts_df['title']\n",
    "    \"\"\"\n",
    "    return podcasts_df[podcasts_df.title == title][\"idx\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZD7LRGxe-ZXb"
   },
   "outputs": [],
   "source": [
    "def recommend(podcast_title, sim_matrix, number_recs=5, pretty_print=True):\n",
    "    \"\"\"given a podcast title & a similarity matrix, return n most similar podcasts\n",
    "        parameters:\n",
    "            podcast_title: (str) must be in podcasts_tf['title]\n",
    "            sim_matrix: (np.array) similarity matrix\n",
    "            number_recs: (int) how many recommendations do you want per title?\n",
    "        returns:\n",
    "            recommendations: (list[str]) list of n most similar podcasts \n",
    "                            according to the similarity matrix\n",
    "    \"\"\"\n",
    "\n",
    "    podcast_id = get_index_from_title(podcast_title)\n",
    "    similar_podcasts =  list(enumerate(sim_matrix[podcast_id]))\n",
    "    sorted_similar_podcast = sorted(similar_podcasts,key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    recommendations = [get_title_from_index(sorted_similar_podcast[i][0]) for i in range(number_recs+2)]\n",
    "    \n",
    "    ### formatting for pretty printing ###\n",
    "    if pretty_print:\n",
    "      print(\"If you liked {}, try: \".format(podcast_title))\n",
    "      for i in recommendations[1:]:\n",
    "          print(\"     {}\".format(i))\n",
    "    \n",
    "    return recommendations[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hdMzyo51-ZXc"
   },
   "outputs": [],
   "source": [
    "# Podcasts we'll use to validate results\n",
    "sample_podcasts = ['The Daily', \"Murder, etc.\",'This American Life', 'Call Her Daddy', 'The Joe Rogan Experience']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96uz3ukA-ZXc"
   },
   "source": [
    "# Bag of Words + Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1RMswK0mQG2"
   },
   "source": [
    "Here, we use the bag of words model to encode the podcast text and use that to generate a cosine similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "iOqY0hXX-ZXd"
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cv_matrix = cv.fit_transform(podcasts_df[\"text\"])\n",
    "cv_cosine_sim = cosine_similarity(cv_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WzP7Z8Q5OLG6",
    "outputId": "82bf0e3e-6399-41c1-8e99-1acfbb115138"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4303, 95787)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at how many words are in our vocabulary:\n",
    "cv_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D8kvHOkviMuR",
    "outputId": "7f8545a1-6198-43f6-aef8-bce9a13f2c65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.0667489 , 0.02251472, ..., 0.00647609, 0.02846787,\n",
       "        0.03677664],\n",
       "       [0.0667489 , 1.        , 0.11828514, ..., 0.02986087, 0.09000942,\n",
       "        0.11970001],\n",
       "       [0.02251472, 0.11828514, 1.        , ..., 0.17455451, 0.17043338,\n",
       "        0.12398095],\n",
       "       ...,\n",
       "       [0.00647609, 0.02986087, 0.17455451, ..., 1.        , 0.21086224,\n",
       "        0.1287162 ],\n",
       "       [0.02846787, 0.09000942, 0.17043338, ..., 0.21086224, 1.        ,\n",
       "        0.13933709],\n",
       "       [0.03677664, 0.11970001, 0.12398095, ..., 0.1287162 , 0.13933709,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's examine our cosine similarity matrix\n",
    "cv_cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N5EvomXPue-k",
    "outputId": "35aa2ab9-b711-4fb1-e3be-445e15a40b3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Daily',\n",
       " 'Murder, etc.',\n",
       " 'This American Life',\n",
       " 'Call Her Daddy',\n",
       " 'The Joe Rogan Experience']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_podcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gKKeEQFg-ZXd",
    "outputId": "73b007b4-24ce-4ca3-898e-fb13b3fb6918"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you liked The Daily, try: \n",
      "     Impeachment Inquiry: Updates from The Washington Post\n",
      "     Impeachment: A Daily Podcast\n",
      "     The Takeaway\n",
      "     Article II: Inside Impeachment\n",
      "     The Daily 202's Big Idea\n",
      "     The 11th Hour with Brian Williams\n",
      "\n",
      "\n",
      "If you liked Murder, etc., try: \n",
      "     Criminology\n",
      "     Murderville\n",
      "     Unsolved Murders: True Crime Stories\n",
      "     Murder Minute\n",
      "     Don't Talk to Strangers\n",
      "     True Crime All The Time Unsolved\n",
      "\n",
      "\n",
      "If you liked This American Life, try: \n",
      "     The Stoop Storytelling Series\n",
      "     The Story Home Children's Audio Stories\n",
      "     Spooky Boo's Scary Story Time\n",
      "     The Story Behind\n",
      "     This is the Gospel Podcast\n",
      "     1001 Heroes, Legends, Histories & Mysteries Podcast\n",
      "\n",
      "\n",
      "If you liked Call Her Daddy, try: \n",
      "     Stiff Socks\n",
      "     Two Judgey Girls\n",
      "     NAKED with Catt Sadler\n",
      "     Slay Girl Slay\n",
      "     Hot Marriage. Cool Parents.\n",
      "     Safe For Work\n",
      "\n",
      "\n",
      "If you liked The Joe Rogan Experience, try: \n",
      "     The Creative Penn Podcast For Writers\n",
      "     1001 Classic Short Stories & Tales\n",
      "     3 Books With Neil Pasricha\n",
      "     The Ground Up Show\n",
      "     1001 Stories For The Road\n",
      "     1001 Heroes, Legends, Histories & Mysteries Podcast\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in sample_podcasts:\n",
    "    recs = recommend(i, cv_cosine_sim)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gS3unzgAao7w",
    "outputId": "7d1df372-edae-41aa-ddea-7bcc56b92997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you liked The Jordan B. Peterson Podcast, try: \n",
      "     Jordan Peterson Archive\n",
      "     Jordan Peterson Interviews & Speeches\n",
      "     AMERICA'S DOCTOR: The Dr. Oz Podcast\n",
      "     Adulting\n",
      "     She Podcast\n",
      "     The Skinny Confidential Him & Her Podcast\n"
     ]
    }
   ],
   "source": [
    "#Try it yourself! \n",
    "your_podcast = \"The Jordan B. Peterson Podcast\" #Replace this with a podcast of your choice!\n",
    "recs = recommend(your_podcast, cv_cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DyuFGZY-ZXe"
   },
   "source": [
    "# TFIDF + Cosine Similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJbzfA8vmE9k"
   },
   "source": [
    "Here, we use tf-idf to encode the podcast text and use that to generate a cosine similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "UvTkeTDr-ZXe"
   },
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()\n",
    "tf_matrix = tf.fit_transform(podcasts_df[\"text\"])\n",
    "tf_cosine_sim = cosine_similarity(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovOEMrZ9O6jW",
    "outputId": "d9d5dbee-9d02-4eac-cea7-bbb6b6eac85d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.01419719, 0.00750197, ..., 0.00185515, 0.00643431,\n",
       "        0.00463919],\n",
       "       [0.01419719, 1.        , 0.03029042, ..., 0.00772847, 0.01713156,\n",
       "        0.02497313],\n",
       "       [0.00750197, 0.03029042, 1.        , ..., 0.0502345 , 0.042034  ,\n",
       "        0.03182914],\n",
       "       ...,\n",
       "       [0.00185515, 0.00772847, 0.0502345 , ..., 1.        , 0.04325737,\n",
       "        0.01595114],\n",
       "       [0.00643431, 0.01713156, 0.042034  , ..., 0.04325737, 1.        ,\n",
       "        0.0208546 ],\n",
       "       [0.00463919, 0.02497313, 0.03182914, ..., 0.01595114, 0.0208546 ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's examine our tf_idf cosine similarity matrix!\n",
    "\n",
    "tf_cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3xutGPIc-ZXe",
    "outputId": "2162eb87-d6cb-44da-de8d-d51a7ddbdbd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you liked The Daily, try: \n",
      "     Impeachment Inquiry: Updates from The Washington Post\n",
      "     The 11th Hour with Brian Williams\n",
      "     The Daily 202's Big Idea\n",
      "     Article II: Inside Impeachment\n",
      "     Impeachment: A Daily Podcast\n",
      "     The Takeaway\n",
      "\n",
      "\n",
      "If you liked Murder, etc., try: \n",
      "     Murder Minute\n",
      "     Criminology\n",
      "     Murderville\n",
      "     Unsolved Murders: True Crime Stories\n",
      "     Don't Talk to Strangers\n",
      "     True Crime All The Time Unsolved\n",
      "\n",
      "\n",
      "If you liked This American Life, try: \n",
      "     Experimental Brewing\n",
      "     1A\n",
      "     Through the Looking Glass: A LOST Retrospective\n",
      "     The Grave Talks | Haunted, Paranormal & Supernatural\n",
      "     Darkness Prevails Podcast | TRUE Horror Stories\n",
      "     BeerSmith Home and Beer Brewing Podcast\n",
      "\n",
      "\n",
      "If you liked Call Her Daddy, try: \n",
      "     hey, girl.\n",
      "     Girls Night with Stephanie May Wilson\n",
      "     Stiff Socks\n",
      "     Fierce Girls\n",
      "     Becoming Something with Jonathan Pokluda\n",
      "     Two Judgey Girls\n",
      "\n",
      "\n",
      "If you liked The Joe Rogan Experience, try: \n",
      "     MILLION DOLLAR LIFE LESSONS\n",
      "     Malcolm Gladwell, Revisionist History: Special Event\n",
      "     The Horror of Dolores Roach\n",
      "     Jordan Peterson Interviews & Speeches\n",
      "     Revisionist History\n",
      "     Ari Shaffir's Skeptic Tank\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in sample_podcasts:\n",
    "    recs = recommend(i, tf_cosine_sim)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBi5dWNibwYe",
    "outputId": "168c8da1-e28b-4da8-d361-b72aeb50d6ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you liked Making Sense with Sam Harris, try: \n",
      "     It's Been a Minute with Sam Sanders\n",
      "     The Kevin Rose Show\n",
      "     The Good Parts with Andy Grammer\n",
      "     The Peter Attia Drive\n",
      "     Science Salon\n",
      "     Common Sense with Dan Carlin\n"
     ]
    }
   ],
   "source": [
    "#Try it yourself! \n",
    "your_podcast = \"Making Sense with Sam Harris\" #Replace this with a podcast of your choice!\n",
    "recs = recommend(your_podcast, tf_cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lhHCXCH-ZXg"
   },
   "source": [
    "# Compare results of the two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rMAvkfQl59c"
   },
   "source": [
    "We want to see whether or not the models tend to agree,\n",
    "and what amount of the total body of podcasts are ever actually recommended (do we solve the long tail problem)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "38Zdcc76-ZXg"
   },
   "outputs": [],
   "source": [
    "def print_compare(pod, num_recs=5):\n",
    "    \"\"\"for a given podcast and number of recommendations\n",
    "        print the recommendations from both tf-idf and cv\n",
    "        unique to tf-idf\n",
    "        and unique to cv\n",
    "    \"\"\"\n",
    "\n",
    "    tf_idf_recs = recommend(pod, tf_cosine_sim, num_recs, pretty_print=False)\n",
    "    cv_recs = recommend(pod, cv_cosine_sim, num_recs, pretty_print=False)\n",
    "\n",
    "    both = list(set(tf_idf_recs).intersection(set(cv_recs)))\n",
    "    unique_to_tf = list(set(tf_idf_recs).difference(set(cv_recs)))\n",
    "    unique_to_cv = list(set(cv_recs).difference(set(tf_idf_recs)))\n",
    "    print(\"Recs for {}: \".format(pod))\n",
    "    \n",
    "    print(\"    Recommended by both tf-idf and cv:\")\n",
    "    for i in both: print(\"         {}\".format(i))\n",
    "\n",
    "    print(\"    Uniqely recommended by tf-idf:\")\n",
    "    for i in unique_to_tf: print(\"         {}\".format(i))\n",
    "\n",
    "    print(\"    Uniqely recommended by cv:\")\n",
    "    for i in unique_to_cv: print(\"         {}\".format(i))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TnOhow7a-ZXh",
    "outputId": "1d801c38-31ea-478e-e441-15923afb29ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recs for The Daily: \n",
      "    Recommended by both tf-idf and cv:\n",
      "         Impeachment Inquiry: Updates from The Washington Post\n",
      "         Article II: Inside Impeachment\n",
      "         The 11th Hour with Brian Williams\n",
      "         Impeachment: A Daily Podcast\n",
      "         The Daily 202's Big Idea\n",
      "         The Takeaway\n",
      "    Uniqely recommended by tf-idf:\n",
      "    Uniqely recommended by cv:\n",
      "\n",
      "\n",
      "Recs for Murder, etc.: \n",
      "    Recommended by both tf-idf and cv:\n",
      "         True Crime All The Time Unsolved\n",
      "         Murderville\n",
      "         Criminology\n",
      "         Unsolved Murders: True Crime Stories\n",
      "         Don't Talk to Strangers\n",
      "         Murder Minute\n",
      "    Uniqely recommended by tf-idf:\n",
      "    Uniqely recommended by cv:\n",
      "\n",
      "\n",
      "Recs for This American Life: \n",
      "    Recommended by both tf-idf and cv:\n",
      "    Uniqely recommended by tf-idf:\n",
      "         Through the Looking Glass: A LOST Retrospective\n",
      "         1A\n",
      "         The Grave Talks | Haunted, Paranormal & Supernatural\n",
      "         Darkness Prevails Podcast | TRUE Horror Stories\n",
      "         Experimental Brewing\n",
      "         BeerSmith Home and Beer Brewing Podcast\n",
      "    Uniqely recommended by cv:\n",
      "         1001 Heroes, Legends, Histories & Mysteries Podcast\n",
      "         The Story Behind\n",
      "         The Story Home Children's Audio Stories\n",
      "         This is the Gospel Podcast\n",
      "         Spooky Boo's Scary Story Time\n",
      "         The Stoop Storytelling Series\n",
      "\n",
      "\n",
      "Recs for Call Her Daddy: \n",
      "    Recommended by both tf-idf and cv:\n",
      "         Two Judgey Girls\n",
      "         Stiff Socks\n",
      "    Uniqely recommended by tf-idf:\n",
      "         Girls Night with Stephanie May Wilson\n",
      "         Becoming Something with Jonathan Pokluda\n",
      "         hey, girl.\n",
      "         Fierce Girls\n",
      "    Uniqely recommended by cv:\n",
      "         Safe For Work\n",
      "         NAKED with Catt Sadler\n",
      "         Hot Marriage. Cool Parents.\n",
      "         Slay Girl Slay\n",
      "\n",
      "\n",
      "Recs for The Joe Rogan Experience: \n",
      "    Recommended by both tf-idf and cv:\n",
      "    Uniqely recommended by tf-idf:\n",
      "         Ari Shaffir's Skeptic Tank\n",
      "         MILLION DOLLAR LIFE LESSONS\n",
      "         Revisionist History\n",
      "         Malcolm Gladwell, Revisionist History: Special Event\n",
      "         The Horror of Dolores Roach\n",
      "         Jordan Peterson Interviews & Speeches\n",
      "    Uniqely recommended by cv:\n",
      "         1001 Heroes, Legends, Histories & Mysteries Podcast\n",
      "         The Creative Penn Podcast For Writers\n",
      "         1001 Classic Short Stories & Tales\n",
      "         The Ground Up Show\n",
      "         3 Books With Neil Pasricha\n",
      "         1001 Stories For The Road\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pod in sample_podcasts: print_compare(pod) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NfIav1MJhI_z",
    "outputId": "e8d26080-a323-4435-b3c5-2f5caeea247e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recs for Happier with Gretchen Rubin: \n",
      "    Recommended by both tf-idf and cv:\n",
      "         Do The Thing, with Whole30's Melissa Urban\n",
      "         Achieve Your Goals with Hal Elrod\n",
      "    Uniqely recommended by tf-idf:\n",
      "         Kalyn’s Coffee Talk\n",
      "         Family Ghosts\n",
      "         Side Hustle School\n",
      "         Happy Hour with Gretchen Geraghty\n",
      "    Uniqely recommended by cv:\n",
      "         minimal-ish: realistic minimalism\n",
      "         The Edge of Sleep\n",
      "         This is Love\n",
      "         Better Than Happy\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try it yourself!\n",
    "\n",
    "your_podcast = \"Happier with Gretchen Rubin\" #Replace this with your podcast \n",
    "print_compare(your_podcast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "uQNrzYWT-ZXh"
   },
   "outputs": [],
   "source": [
    "def coverage(model_name, sim_matrix, num_recs=10):\n",
    "    \"\"\"Track what % of the overall library of podcasts\n",
    "        was ever actually recommended, when we serve\n",
    "        10 recs for each podcast in the library\n",
    "\n",
    "        parameters:\n",
    "          model_name: (str) either 'tf-idf' or 'cv'\n",
    "                    should correspond to the passed sim_matrix \n",
    "          sim_matrix: (np.array) an item-item similarity matrix\n",
    "          num_recs: how many recs for each item in library?\n",
    "        returns:\n",
    "          indices: (np.array) recommended podcast indices\n",
    "    \"\"\"\n",
    "    indices = np.argpartition(sim_matrix, -num_recs, axis=1)[:,-num_recs:]\n",
    "    \n",
    "    #calculating coverage:\n",
    "    recommended = set(list(itertools.chain(*indices)))\n",
    "    coverage = (len(recommended)/indices.shape[0])*100\n",
    "\n",
    "    print(\"Stats for {} Model with {} recs\".format(model_name, num_recs))\n",
    "    print(\"    Coverage: {} %\".format(coverage))\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yuXMQRft-ZXh",
    "outputId": "533af9f3-b7fd-4a8f-a5a2-1d19ea240c73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for CountVectorizer Model with 5 recs\n",
      "    Coverage: 100.0 %\n",
      "Stats for tf-idf Model with 5 recs\n",
      "    Coverage: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "cv_recs_10 = coverage(\"CountVectorizer\", cv_cosine_sim, 5)\n",
    "tf_idf_recs_10 = coverage(\"tf-idf\", tf_cosine_sim, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDE3KrYNT4vI"
   },
   "source": [
    "# Implementing Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXOK355G-ZXi"
   },
   "source": [
    "## Generating Fake User Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8T96RWnlP8t"
   },
   "source": [
    "We want to create users that have preferences.\n",
    "Each of them randomly rates between 5-20 randomly selected podcasts on a scale from 1-5. \n",
    "This is a non-realistic way to generate fake user ratings (as most users like similar things, and have a pattern to how they rate things). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "4P4Z-47S-ZXi"
   },
   "outputs": [],
   "source": [
    "def generate_user_ratings(users_count):\n",
    "    \"\"\"generates fake user ratings\n",
    "      parameters:\n",
    "        users_count: (int) how many fake users to generate\n",
    "      returns:\n",
    "        users: (pd.DataFrame) table of user, podcast, & rating    \n",
    "    \"\"\"\n",
    "    \n",
    "    user_ratings = []\n",
    "    for idx, user in enumerate(np.arange(0,users_count)):\n",
    "        ratings = []\n",
    "        quantity_rated = np.random.randint(5,21)\n",
    "        reviewed = set()\n",
    "        \n",
    "        for i in np.arange(quantity_rated):\n",
    "            podcast =  np.random.randint(0, podcasts_df.shape[0])\n",
    "            title = get_title_from_index(podcast)\n",
    "            \n",
    "            # don't want the same user to review \n",
    "            # the same podcast multiple times\n",
    "            while (podcast in reviewed):\n",
    "                podcast =  np.random.randint(0, podcasts_df.shape[0]+1)\n",
    "            reviewed.add(podcast)\n",
    "\n",
    "            rating = np.random.randint(1,6)\n",
    "            ratings.append([idx, podcast, rating, title])\n",
    "        \n",
    "        user_df = pd.DataFrame(ratings, \\\n",
    "                          columns=['user_id', 'podcast_idx', 'rating', 'podcast_title'])\n",
    "        user_ratings.append(user_df)\n",
    "    return pd.concat(user_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "nOGOKjsslPMH"
   },
   "outputs": [],
   "source": [
    "def checkUserProfile(user_idx, pretty_print=True):\n",
    "  \"\"\"For a given user id, create a profile including 3 attributes\n",
    "\n",
    "    parameters:\n",
    "      user_id: (int) user id\n",
    "      print: (boolean) whether or not printed outcomes are desired\n",
    "    \n",
    "    returns:\n",
    "      user_profile: (dict) contains 3 attributes of a user profile\n",
    "  \"\"\"\n",
    "  user_id=user_idx\n",
    "  user_reviews = usr.query('user_id==@user_id') \\\n",
    "          .sort_values('rating', ascending=False)\n",
    "  \n",
    "  user_profile = {'no_reviews' : user_reviews.shape[0], \n",
    "                  'top_5_shows' : user_reviews['podcast_title'].iloc[:5].to_list(), \n",
    "                  'ave_rating' : user_reviews['rating'].mean() }\n",
    "  \n",
    "  \n",
    "  #### formatting for pretty printing ###  \n",
    "  if pretty_print:\n",
    "    print(f\"User #{user_id} Profile:\")\n",
    "    print(f\"{user_profile['no_reviews']} reviews\")\n",
    "    print(f\"Mean rating: {user_profile['ave_rating']} stars\")\n",
    "    print(f\"Top 5 shows:\")\n",
    "    \n",
    "    for show in user_profile['top_5_shows']:\n",
    "      print(f\"       {show}\")\n",
    "    print(\"                ..            \")\n",
    "  \n",
    "  return user_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "gQcI0Xdt-ZXj"
   },
   "outputs": [],
   "source": [
    "num_users = 1000\n",
    "usr = generate_user_ratings(num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0D8G8nLj2Jc",
    "outputId": "a432a13b-02b5-48f9-eb57-c5834df94083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User #613 Profile:\n",
      "11 reviews\n",
      "Mean rating: 3.4545454545454546 stars\n",
      "Top 5 shows:\n",
      "       Encyclopedia Womannica\n",
      "       The Late-Round Podcast\n",
      "       The World of Phil Hendrie\n",
      "       The SelfWork Podcast\n",
      "       French Podcast\n",
      "                ..            \n"
     ]
    }
   ],
   "source": [
    "#investigate a random user!\n",
    "my_random_user = np.random.randint(0, num_users)\n",
    "profile = checkUserProfile(my_random_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-wfKWg9W-J1"
   },
   "source": [
    "## Implement ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qOaLB11miZd"
   },
   "source": [
    "Now that we have our user rating data, we can implement collaborative filtering to generate recommendations based on user similarity. We specifically used the pyspark implementation of ALS Matrix Factorization with root mean squared error. \n",
    "\n",
    "We used a pyspark implementation of ALS code as published by [Jeffrey Chiang](https://github.com/chiang9/Medium_blog/blob/main/ALS_model/movielen%20ALS.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "mYnRorb6-ZXj"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "     \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgApYMLUMsHH"
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(usr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DThK-CVMKnT5"
   },
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7,0.3],111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrTbKwJQK4Vi"
   },
   "outputs": [],
   "source": [
    "# we use the cross validator to tune the hyperparameters\n",
    "als = ALS(\n",
    "         userCol=\"user_id\", \n",
    "         itemCol=\"podcast_idx\",\n",
    "         ratingCol=\"rating\", \n",
    "         coldStartStrategy=\"drop\" \n",
    ")\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [10, 100]) \\\n",
    "            .addGrid(als.regParam, [.1]) \\\n",
    "            .addGrid(als.maxIter, [10]) \\\n",
    "            .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "           metricName=\"rmse\", \n",
    "           labelCol=\"rating\", \n",
    "           predictionCol=\"prediction\")\n",
    "\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3, parallelism = 6)\n",
    "model = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g2N3O2VjODsp",
    "outputId": "f091adb3-871f-48de-9b61-3df659cee728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of latent factors Used = 100\n"
     ]
    }
   ],
   "source": [
    "best_model = model.bestModel\n",
    "\n",
    "print(f\"# of latent factors Used = {best_model._java_obj.parent().getRank()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PgdGDtPEOF8m",
    "outputId": "fdeefbb2-201d-42b9-d87e-cd5868b8ec75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 2.5522706293182815\n"
     ]
    }
   ],
   "source": [
    "prediction = best_model.transform(test)\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print(f'RMSE = {rmse}')\n",
    "\n",
    "# we can get the user latent factors and item latent factors from the model\n",
    "user_latent_features = best_model.userFactors\n",
    "item_latent_features = best_model.itemFactors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Fvq-IS6QF9H"
   },
   "outputs": [],
   "source": [
    "user_recs = best_model.recommendForAllUsers(3)\n",
    "user_recs_pandas = user_recs.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIeOnfSzQkjS"
   },
   "outputs": [],
   "source": [
    "def checkUserRecommendations(user_row_idx, pretty_print=True):\n",
    "  \"\"\"Print each user's profile\n",
    "    and recommended future podcasts/predicted ratings\n",
    "\n",
    "    parameters:\n",
    "      user_row: (int) index of row from user_recs dataframe\n",
    "      print: (boolean) whether or not printed outcomes are desired\n",
    "    \n",
    "    returns:\n",
    "      user_recs: (list) recommended podcast titles\n",
    "  \"\"\"\n",
    "  user_row = user_recs_pandas.iloc[user_row_idx]\n",
    "  user_id = user_row['user_id']\n",
    "  user_profile = checkUserProfile(user_id)\n",
    "  \n",
    "  user_recs=[]\n",
    "  for rec in user_row['recommendations']:\n",
    "    rec_idx = rec.__getitem__('podcast_idx')\n",
    "    rec_title = get_title_from_index(rec_idx)\n",
    "    user_recs.append(rec_title)\n",
    "\n",
    "  #### formatting for pretty printing ###  \n",
    "  if pretty_print: \n",
    "    print(\"We recommend the following: \")\n",
    "    for rec_title in user_recs:\n",
    "      print(f\"       {rec_title}\")\n",
    "  print(\"\\n\")\n",
    "  return user_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KO7xKR9xdzVM",
    "outputId": "8d35b2cd-6feb-45e2-acb8-7169fef2663a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User #343 Profile:\n",
      "10 reviews\n",
      "Mean rating: 3.6 stars\n",
      "Top 5 shows:\n",
      "       Voddie Baucham on SermonAudio\n",
      "       Magic: The Gathering Drive to Work Podcast\n",
      "       True Crime All The Time\n",
      "       Scam Goddess\n",
      "       Kubernetes Podcast from Google\n",
      "                ..            \n",
      "We recommend the following: \n",
      "       Impeachment Today\n",
      "       SOFREP Radio\n",
      "       Planet Money\n",
      "\n",
      "\n",
      "User #898 Profile:\n",
      "15 reviews\n",
      "Mean rating: 3.4 stars\n",
      "Top 5 shows:\n",
      "       Our Portland with Sarah Iannarone\n",
      "       Radiolab\n",
      "       Kalila Stormfire's Economical Magick Services\n",
      "       Regenerative Agriculture Podcast\n",
      "       Noodle Loaf\n",
      "                ..            \n",
      "We recommend the following: \n",
      "       Wonders of the World\n",
      "       Weeknight Kitchen with Melissa Clark\n",
      "       CuriosiD\n",
      "\n",
      "\n",
      "User #17 Profile:\n",
      "15 reviews\n",
      "Mean rating: 3.0 stars\n",
      "Top 5 shows:\n",
      "       River to River\n",
      "       Stanford Steve & The Bear\n",
      "       The Unexpectables\n",
      "       Directionally Challenged\n",
      "       The Allusionist\n",
      "                ..            \n",
      "We recommend the following: \n",
      "       Speaking of Psychology\n",
      "       The Amelia Project\n",
      "       What Should I Read Next?\n",
      "\n",
      "\n",
      "User #29 Profile:\n",
      "13 reviews\n",
      "Mean rating: 3.076923076923077 stars\n",
      "Top 5 shows:\n",
      "       Statehouse Blend Missouri\n",
      "       Mile Marker 181\n",
      "       Business Wars Daily\n",
      "       Savvy Painter Podcast with Antrese Wood\n",
      "       Collider Movie Talk\n",
      "                ..            \n",
      "We recommend the following: \n",
      "       The ThomaHawk Show\n",
      "       WSJ Opinion: Potomac Watch\n",
      "       Fireside Mystery Theatre\n",
      "\n",
      "\n",
      "User #488 Profile:\n",
      "15 reviews\n",
      "Mean rating: 3.3333333333333335 stars\n",
      "Top 5 shows:\n",
      "       Laughs from the Past\n",
      "       Meditation Oasis\n",
      "       The NFX Podcast\n",
      "       No Putts Given\n",
      "       Sneak Attack!\n",
      "                ..            \n",
      "We recommend the following: \n",
      "       The Wilderness\n",
      "       Christopher Kimball’s Milk Street Radio\n",
      "       The Liturgists Podcast\n",
      "\n",
      "\n",
      "User #647 Profile:\n",
      "16 reviews\n",
      "Mean rating: 2.8125 stars\n",
      "Top 5 shows:\n",
      "       What Makes a Killer\n",
      "       Ear Snacks\n",
      "       Stay Tuned with Preet\n",
      "       Wine for Normal People\n",
      "       The Michael Knowles Show\n",
      "                ..            \n",
      "We recommend the following: \n",
      "       The Podcast History Of Our World\n",
      "       Worldly\n",
      "       Unconcluded\n",
      "\n",
      "\n",
      "User #732 Profile:\n",
      "11 reviews\n",
      "Mean rating: 3.3636363636363638 stars\n",
      "Top 5 shows:\n",
      "       The Creeping Hour\n",
      "       The Box of Oddities\n",
      "       Ghost of a Podcast\n",
      "       Thirty Minutes with The Perrys\n",
      "       The mindbodygreen Podcast\n",
      "                ..            \n",
      "We recommend the following: \n",
      "       Rhythms for Life\n",
      "       The Land and the Book\n",
      "       DELETE THIS\n",
      "\n",
      "\n",
      "User #69 Profile:\n",
      "19 reviews\n",
      "Mean rating: 3.3157894736842106 stars\n",
      "Top 5 shows:\n",
      "       I Don't Have Enough FAITH to Be an ATHEIST\n",
      "       Til Death Do Us Blart\n",
      "       Song Confessional\n",
      "       NASW Social Work Talks\n",
      "       Fat Mascara\n",
      "                ..            \n",
      "We recommend the following: \n",
      "       RZIM: Thinking Out Loud Broadcasts\n",
      "       Bookworm\n",
      "       The Ziglar Show\n",
      "\n",
      "\n",
      "User #192 Profile:\n",
      "6 reviews\n",
      "Mean rating: 3.8333333333333335 stars\n",
      "Top 5 shows:\n",
      "       Switchblade Sisters\n",
      "       Thinking Sideways Podcast\n",
      "       PB&J: Politics with Brian & Jake\n",
      "       LinuxAcademy.com\n",
      "       Marvel Cinematic Universe Podcast\n",
      "                ..            \n",
      "We recommend the following: \n",
      "       Carrier\n",
      "       Nora En Pure - Purified Radio\n",
      "       Marketplace\n",
      "\n",
      "\n",
      "User #438 Profile:\n",
      "8 reviews\n",
      "Mean rating: 3.125 stars\n",
      "Top 5 shows:\n",
      "       Wine for Normal People\n",
      "       HBR IdeaCast\n",
      "       The Shameless Podcast\n",
      "       Voice Over Body Shop\n",
      "       Gilbert Gottfried's Amazing Colossal Podcast\n",
      "                ..            \n",
      "We recommend the following: \n",
      "       Newshour\n",
      "       MAPS Podcast\n",
      "       Philosophize This!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking out the profiles & recommendations \n",
    "# for 10 random users\n",
    "\n",
    "for i in np.random.randint(0, len(user_recs_pandas), 10):\n",
    "  checkUserRecommendations(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJGewltVjWM2",
    "outputId": "69a84966-5e83-4c1e-c0bb-607b15c0de27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User #207 Profile:\n",
      "15 reviews\n",
      "Mean rating: 2.2 stars\n",
      "Top 5 shows:\n",
      "       Attention HellMart Shoppers!\n",
      "       The Brilliant Idiots\n",
      "       Newt's World\n",
      "       The 2 Robbies\n",
      "       The Stronger By Science Podcast\n",
      "                ..            \n",
      "We recommend the following: \n",
      "       The Brilliant Idiots\n",
      "       Attention HellMart Shoppers!\n",
      "       Newt's World\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#try it yourself!\n",
    "my_random_user = np.random.randint(0, len(user_recs_pandas))\n",
    "recs = checkUserRecommendations(my_random_user)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
